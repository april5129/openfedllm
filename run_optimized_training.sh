#!/bin/bash

# é’ˆå¯¹ä½å†…å­˜æœåŠ¡å™¨ä¼˜åŒ–çš„è”é‚¦å­¦ä¹ è®­ç»ƒè„šæœ¬
# é€‚ç”¨äº3.7GBå†…å­˜çš„è…¾è®¯äº‘æœåŠ¡å™¨

echo "ğŸš€ å¯åŠ¨ä¼˜åŒ–çš„è”é‚¦å­¦ä¹ è®­ç»ƒï¼ˆä½å†…å­˜é…ç½®ï¼‰..."

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH="/root/openfedllm:$PYTHONPATH"
export RAY_DISABLE_IMPORT_WARNING=1

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p /root/openfedllm/output/optimized_training

# æ£€æŸ¥Rayé›†ç¾¤çŠ¶æ€
echo "æ£€æŸ¥Rayé›†ç¾¤çŠ¶æ€..."
ray status

# è¿è¡Œä¼˜åŒ–çš„åˆ†å¸ƒå¼è®­ç»ƒ
echo "å¼€å§‹ä¼˜åŒ–çš„åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ è®­ç»ƒ..."
python3 runner/FedFT/distributed_fedavg_runner.py \
    --model_name_or_path "microsoft/DialoGPT-medium" \
    --dataset_name "lucasmccabe-lmi/CodeAlpaca-20k" \
    --num_rounds 10 \
    --num_clients 2 \
    --sample_clients 1 \
    --learning_rate 5e-5 \
    --batch_size 4 \
    --seq_length 256 \
    --num_train_epochs 1 \
    --max_steps 50 \
    --output_dir "/root/openfedllm/output/optimized_training" \
    --use_peft \
    --peft_lora_r 4 \
    --peft_lora_alpha 8 \
    --template "alpaca" \
    --dataset_sample 1000 \
    --gradient_accumulation_steps 2 \
    --save_steps 25 \
    --logging_steps 10 \
    --warmup_steps 10 \
    --load_in_8bit

echo "âœ… ä¼˜åŒ–çš„è”é‚¦å­¦ä¹ è®­ç»ƒå®Œæˆï¼"

# åœæ­¢Rayé›†ç¾¤
echo "åœæ­¢Rayé›†ç¾¤..."
ray stop

echo "ğŸ“Š è®­ç»ƒç»“æœä¿å­˜åœ¨: /root/openfedllm/output/optimized_training" 