# OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning

**OpenFedLLM** is an open-source research-use codebase for training *Large Language Models (LLM)* via federated learning (accepted by **KDD 2024**). Please check our [paper](https://arxiv.org/abs/2402.06954) for details and the corresponding empirical study.

OpenFedLLM includes the following key features:
- 7 **federated learning** algorithms (e.g., *FedAvg*, *FedProx*, *SCAFFOLD*, *FedAvgM*, etc.).
- 2 **LLM training** algorithms, including instruction tuning (i.e. *SFT*) and value alignment (i.e., *DPO*).
- 30+ **evaluation metrics** covering *general capabilities*, *medical QA*, *financial QA*, *code generation*, *math solving*, and more.


![intro](doc/assets/openfedllm-intro.png)

## åˆ†å¸ƒå¼éƒ¨ç½²

ä¸ºäº†æ”¯æŒåœ¨å¤šå°æœºå™¨ä¸Šåˆ†å¸ƒå¼éƒ¨ç½²ï¼Œæˆ‘ä»¬æä¾›äº†æ‹†åˆ†çš„æœåŠ¡ç«¯å’Œå®¢æˆ·ç«¯ä»£ç ï¼š

- `server.py`: æœåŠ¡ç«¯ä»£ç ï¼Œè´Ÿè´£å…¨å±€æ¨¡å‹èšåˆå’Œåˆ†å‘
- `client.py`: å®¢æˆ·ç«¯ä»£ç ï¼Œè´Ÿè´£æœ¬åœ°æ¨¡å‹è®­ç»ƒ

### éƒ¨ç½²æ¶æ„

åœ¨4å°æœºå™¨ä¸Šçš„éƒ¨ç½²æ¶æ„å¦‚ä¸‹ï¼š

1. ä¸€å°æœºå™¨ä½œä¸ºæœåŠ¡ç«¯ï¼ˆå‚æ•°æœåŠ¡å™¨ï¼‰
2. ä¸‰å°æœºå™¨ä½œä¸ºå®¢æˆ·ç«¯ï¼ˆè®­ç»ƒèŠ‚ç‚¹ï¼‰

### éƒ¨ç½²æ­¥éª¤

#### 1. ç¯å¢ƒå‡†å¤‡

åœ¨æ‰€æœ‰æœºå™¨ä¸Šå®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

#### 2. æ•°æ®å‡†å¤‡

åœ¨æ¯å°å®¢æˆ·ç«¯æœºå™¨ä¸Šå‡†å¤‡æœ¬åœ°æ•°æ®é›†ã€‚ç¡®ä¿æ¯å°æœºå™¨ä¸Šçš„æ•°æ®æ˜¯ä¸åŒçš„ï¼Œä»¥ç¬¦åˆè”é‚¦å­¦ä¹ çš„éšç§ä¿æŠ¤ç‰¹æ€§ã€‚

#### 3. é…ç½®ä¿®æ”¹

ä¿®æ”¹ `config.py` æ–‡ä»¶ä¸­çš„å‚æ•°ä»¥é€‚åº”åˆ†å¸ƒå¼éƒ¨ç½²ï¼š

- `num_clients`: è®¾ç½®ä¸º3ï¼ˆå®¢æˆ·ç«¯æ•°é‡ï¼‰
- `sample_clients`: è®¾ç½®ä¸ºå‚ä¸æ¯è½®è®­ç»ƒçš„å®¢æˆ·ç«¯æ•°é‡

#### 4. æœåŠ¡ç«¯å¯åŠ¨

åœ¨æœåŠ¡ç«¯æœºå™¨ä¸Šè¿è¡Œï¼š

```bash
# ä½¿ç”¨HTTPé€šä¿¡ç‰ˆæœ¬
python server_http.py

# æˆ–ä½¿ç”¨æä¾›çš„å¯åŠ¨è„šæœ¬
run_server.bat
```

#### 5. å®¢æˆ·ç«¯å¯åŠ¨

åœ¨æ¯å°å®¢æˆ·ç«¯æœºå™¨ä¸Šè¿è¡Œï¼š

```bash
# ä½¿ç”¨HTTPé€šä¿¡ç‰ˆæœ¬
python client_http.py

# æˆ–ä½¿ç”¨æä¾›çš„å¯åŠ¨è„šæœ¬
run_client.bat
```

### é€šä¿¡å®ç°

æˆ‘ä»¬æä¾›äº†åŸºäºHTTPçš„é€šä¿¡å®ç°ç¤ºä¾‹ï¼š

1. **HTTP/REST API**: ä½¿ç”¨Flaskå®ç°æœåŠ¡ç«¯APIï¼Œå®¢æˆ·ç«¯é€šè¿‡HTTPè¯·æ±‚ä¸æœåŠ¡ç«¯é€šä¿¡
   - æœåŠ¡ç«¯ä»£ç : `server_http.py`
   - å®¢æˆ·ç«¯ä»£ç : `client_http.py`
   - å¯åŠ¨è„šæœ¬: `run_server.bat` å’Œ `run_client.bat`

å…¶ä»–å¯é€‰çš„é€šä¿¡æœºåˆ¶ï¼š

1. **Socketé€šä¿¡**: ä½¿ç”¨Pythonçš„socketåº“å®ç°è‡ªå®šä¹‰é€šä¿¡åè®®
2. **æ¶ˆæ¯é˜Ÿåˆ—**: ä½¿ç”¨RabbitMQã€Kafkaç­‰æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ

### æ³¨æ„äº‹é¡¹

1. ç¡®ä¿æ‰€æœ‰æœºå™¨åœ¨åŒä¸€ç½‘ç»œä¸­ï¼Œèƒ½å¤Ÿäº’ç›¸é€šä¿¡
2. é…ç½®é˜²ç«å¢™è§„åˆ™ï¼Œå¼€æ”¾å¿…è¦çš„ç«¯å£
3. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®ä½¿ç”¨åŠ å¯†é€šä¿¡ä»¥ä¿æŠ¤æ¨¡å‹å‚æ•°
4. æ ¹æ®å®é™…ç¡¬ä»¶é…ç½®è°ƒæ•´è®­ç»ƒå‚æ•°ï¼ˆbatch_sizeã€learning_rateç­‰ï¼‰

## NewsğŸ”¥
- **2024-09:** FedLLM-Bench is accepted by **NeurIPS 2024** Datasets and Benchmarks Track!
- **2024-06:** We released the first realistic benchmark for FedLLM: FedLLM-Bench. Check the [Paper](https://arxiv.org/pdf/2406.04845) | [Code](https://github.com/rui-ye/FedLLM-Bench).
- **2024-05:** OpenFedLLM is accepted by **KDD 2024**!.

## Setup

Clone the repo, submodules and install the required packages.

```
git clone --recursive --shallow-submodules https://github.com/rui-ye/OpenFedLLM.git
cd OpenFedLLM
conda create -n fedllm python=3.10
conda activate fedllm
pip install -r requirements.txt
source setup.sh
```

## Training

We provide training scripts under `training_scripts/`. Try them out from the top-level directory of this repository.

### Federated Instruction Tuning

The training script is in `training_scripts/run_sft.sh`.

```
CUDA_VISIBLE_DEVICES=1 python main_sft.py \
 --model_name_or_path "meta-llama/Llama-2-7b-hf" \
 --dataset_name "vicgalle/alpaca-gpt4" \
 --dataset_sample 20000 \
 --fed_alg "fedavg" \
 --num_clients 20 \
 --sample_clients 2 \
 --max_steps 10 \
 --num_rounds 200 \
 --batch_size 16 \
 --gradient_accumulation_steps 1 \
 --seq_length 512 \
 --peft_lora_r 32 \
 --peft_lora_alpha 64 \
 --use_peft \
 --load_in_8bit \
 --output_dir "./output" \
 --template "alpaca" \
```

Key arguments:

- `model_name_or_path`: the name or local location of your base model
- `template`: template for chatting. Define your own template in `utils/template.py`.
- `dataset_name`: the name of dataset. You may modify `utils/process_dataset.py` if your interested dataset has not been supported.
- `dataset_sample`: needed if you want to sample a specific number of samples from the original dataset.
- `fed_alg`: the name of federated learning algorithm
- `num_clients`/sample_clients: `num_clients` clients in total, `sample_clients` clients for each round
- `max_steps`: the number of model update steps for one client at each round.

### Federated Value Alignment

The training script is in `training_scripts/run_dpo.sh`.

```
python main_dpo.py --template "vicuna_v1.1"
```

Note that the main difference between the usage of `main_sft.py` and `main_dpo.py` lies in the `template` argument. We plan to make them consistent in the future.
- For SFT, templates are defined in `utils/template.py`
- For DPO, templates are defined in `utils/conversation.py`

## Evaluation

Evaluation codes are put in `evaluation/` directory. Most of our evaluations follow existing high-incluence open-source repos. Please refer to each sub-directory for the corresponding detailed README and running script.

For example, `evaluation/open_ended/` include open-ended evaluations on three benchmarks, covering MT-Bench, Vicuna Bench, and AdvBench; see [README.md](evaluation/open_ended/README.md).

## Citation

Please cite our paper if you find the repository helpful.

```
@inproceedings{ye2024openfedllm,
  title={Openfedllm: Training large language models on decentralized private data via federated learning},
  author={Ye, Rui and Wang, Wenhao and Chai, Jingyi and Li, Dihan and Li, Zexi and Xu, Yinda and Du, Yaxin and Wang, Yanfeng and Chen, Siheng},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6137--6147},
  year={2024}
}
```
