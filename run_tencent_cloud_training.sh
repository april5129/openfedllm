#!/bin/bash

# è…¾è®¯äº‘æœåŠ¡å™¨ä¸“ç”¨åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ è¿è¡Œè„šæœ¬

echo "ğŸš€ åœ¨è…¾è®¯äº‘æœåŠ¡å™¨ä¸Šå¯åŠ¨åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ è®­ç»ƒ..."

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH="/root/openfedllm:$PYTHONPATH"
export RAY_DISABLE_IMPORT_WARNING=1

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p /root/openfedllm/output/tencent_cloud_training

# å¯åŠ¨Rayé›†ç¾¤
echo "å¯åŠ¨Rayé›†ç¾¤..."
ray start --head --port=6379 \
    --dashboard-host=0.0.0.0 --dashboard-port=8265 \
    --object-store-memory=2000000000 \
    --memory=4000000000

# ç­‰å¾…é›†ç¾¤å¯åŠ¨
sleep 10

# æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
echo "Rayé›†ç¾¤çŠ¶æ€ï¼š"
ray status

# è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
echo "å¼€å§‹åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ è®­ç»ƒ..."
python3 runner/FedFT/distributed_fedavg_runner.py \
    --model_name_or_path "meta-llama/Llama-2-7b-hf" \
    --dataset_name "lucasmccabe-lmi/CodeAlpaca-20k" \
    --num_rounds 50 \
    --num_clients 4 \
    --sample_clients 2 \
    --learning_rate 2e-5 \
    --batch_size 8 \
    --seq_length 512 \
    --num_train_epochs 1 \
    --max_steps 100 \
    --output_dir "/root/openfedllm/output/tencent_cloud_training" \
    --use_peft \
    --peft_lora_r 8 \
    --peft_lora_alpha 16 \
    --template "alpaca" \
    --dataset_sample 5000

echo "âœ… åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ è®­ç»ƒå®Œæˆï¼"

# åœæ­¢Rayé›†ç¾¤
ray stop

echo "ğŸ“Š è®­ç»ƒç»“æœä¿å­˜åœ¨: /root/openfedllm/output/tencent_cloud_training"
echo "ğŸ“ˆ æŸ¥çœ‹Ray Dashboard: http://106.52.36.202:8265" 